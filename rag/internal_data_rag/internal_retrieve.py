# -*- coding: utf-8 -*-
"""
RAG ë¬¸ì„œ ê²€ìƒ‰ & ë‹µë³€ ì„œë¹„ìŠ¤ (LangChain + Chroma)
- ê¶Œì¥ì‚¬í•­ ë°˜ì˜ ë²„ì „

ë³€ê²½ í•µì‹¬
1) ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ(MAX_CONTEXT_CHARS) ì¶”ê°€ë¡œ í”„ë¡¬í”„íŠ¸ ì´ˆê³¼ ë°©ì§€
2) ì•ˆì • ìœ ì‚¬ë„ ë³€í™˜: similarity = 1 / (1 + distance) (ì½”ì‚¬ì¸ ê±°ë¦¬ ê°€ì •)
3) í—¬ìŠ¤ì²´í¬ ì‹œ private ì†ì„± ì ‘ê·¼ ì œê±° â†’ ì‹¤ì œ ê²€ìƒ‰ ì‹œë„ë¡œ ì²´í¬
4) Document ì„í¬íŠ¸ ê²½ë¡œ ìµœì‹ í™” (langchain_core.documents)
5) CHROMA_PATH ì ˆëŒ€ ê²½ë¡œí™”(ë¡œê·¸ í‘œì‹œ) + í™˜ê²½ë³€ìˆ˜ í†µì¼
6) ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›(ë©”ì„œë“œ ì¸ì model)
7) ì»¨í…ìŠ¤íŠ¸ íŠ¸ë ì¼€ì´ì…˜ ì‹œ ìœ ì‚¬ë„ ìˆœ ì •ë ¬ í›„ ëˆ„ì  ë°”ì´íŠ¸ ì»·
8) ë¡œê·¸ ë©”ì‹œì§€ ê°œì„ 
"""

import os
from typing import List, Tuple

from dotenv import load_dotenv

# LangChain - LLM/Embeddings/VectorStore/Prompt/Parser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.documents import Document  # âœ… ìµœì‹  ê²½ë¡œ

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
CHROMA_PATH = os.path.abspath(CHROMA_PATH)  # âœ… ì ˆëŒ€ ê²½ë¡œí™” (ë¡œê·¸ ê°€ì‹œì„±)
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")  # âœ… ìš´ì˜ ê¸°ë³¸ê°’ ê¶Œì¥
MAX_CONTEXT_CHARS = int(os.getenv("MAX_CONTEXT_CHARS", "12000"))  # âœ… í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ë² ë”© í•¨ìˆ˜ (OpenAI Embeddings ë˜í¼)
_embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

# ë²¡í„°ìŠ¤í† ì–´ (Chroma ë˜í¼)
# - persist_directory: CHROMA_PATH
# - collection_name: COLLECTION_NAME
_vectorstore = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=_embeddings,
    persist_directory=CHROMA_PATH,
)

# LLM (ChatOpenAI ë˜í¼)
_llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)

# í”„ë¡¬í”„íŠ¸ (ì‹œìŠ¤í…œ+ì‚¬ìš©ì)
_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
            "ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ëª¨ë¥¸ë‹¤'ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”. "
            "ê°€ëŠ¥í•œ í•œ ì¶œì²˜ ë¬¸ì„œëª…ê³¼ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”.",
        ),
        (
            "user",
            "ì§ˆë¬¸: {question}\n\n"
            "ì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}",
        ),
    ]
)

# ì¶œë ¥ íŒŒì„œ
_parser = StrOutputParser()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _stable_similarity(distance: float) -> float:
    """ì½”ì‚¬ì¸ ê±°ë¦¬ ê°€ì • ì‹œ ì•ˆì •ì  ìœ ì‚¬ë„ ë³€í™˜: 1 / (1 + d)."""
    try:
        d = float(distance)
    except Exception:
        d = 0.0
    if d < 0:
        d = 0.0
    return 1.0 / (1.0 + d)


def _truncate_context_blocks(blocks: List[Tuple[str, dict]], max_chars: int) -> str:
    """ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, max_chars ê¹Œì§€ ëˆ„ì í•˜ì—¬ ë¬¸ìì—´ êµ¬ì„±.
    blocks: [(doc, meta)] with meta["similarity_score"] ì¡´ì¬ ê°€ì •
    """
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœ ì •ë ¬
    sorted_blocks = sorted(
        blocks,
        key=lambda x: float(x[1].get("similarity_score", 0.0)),
        reverse=True,
    )

    acc: List[str] = []
    total = 0
    sep = "\n\n---\n\n"

    for doc, meta in sorted_blocks:
        header = f"[ì¶œì²˜: {meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} / ì²­í¬: {meta.get('chunk_idx', 'N/A')}]\n"
        block = header + doc
        add_len = len(block) + (len(sep) if acc else 0)
        if total + add_len > max_chars:
            break
        if acc:
            acc.append(sep)
            total += len(sep)
        acc.append(block)
        total += len(block)

    return "".join(acc)


class RetrieveService:
    """ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ (LangChain ë²„ì „, ê¶Œì¥ì‚¬í•­ ë°˜ì˜)"""

    def __init__(self):
        self.vectorstore = _vectorstore
        self.llm = _llm
        self.prompt = _prompt
        self.parser = _parser

    # ========================= ë¬¸ì„œ ê²€ìƒ‰ =========================

    def retrieve_documents(self, query: str, top_k: int = 3) -> List[Tuple[str, dict]]:
        """
        LangChainì˜ vectorstore ë˜í¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰.
        similarity_search_with_scoreë¥¼ ì‚¬ìš©í•´ ì ìˆ˜(distance)ë¥¼ í•¨ê»˜ ë°›ìŠµë‹ˆë‹¤.
        (Chromaì˜ scoreëŠ” ë³´í†µ 'cosine distance'ë¡œ, ê°’ì´ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬.)
        """
        try:
            print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰: '{query}' (ìƒìœ„ {top_k}ê°œ)")

            results: List[Tuple[Document, float]] = self.vectorstore.similarity_search_with_score(
                query, k=top_k
            )
            if not results:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            contexts: List[Tuple[str, dict]] = []
            print(f"âœ… {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            for i, (doc, distance) in enumerate(results, start=1):
                similarity = _stable_similarity(distance)  # âœ… ì•ˆì • ìœ ì‚¬ë„ ë³€í™˜
                meta = dict(doc.metadata or {})
                meta["similarity_score"] = similarity  # ë³´ê³ /ì •ë ¬ìš©
                preview = (doc.page_content[:80] + "...") if len(doc.page_content) > 80 else doc.page_content
                print(
                    f"  [Rank {i}] ìœ ì‚¬ë„={similarity:.4f}, "
                    f"source={meta.get('source')}, chunk={meta.get('chunk_idx')}"
                )
                print(f"          ë‚´ìš©: {preview}")
                contexts.append((doc.page_content, meta))

            return contexts
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    # ========================= ë‹µë³€ ìƒì„± =========================

    def generate_answer(
        self, query: str, contexts: List[Tuple[str, dict]], model: str | None = None
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ì…í•´ LLM í˜¸ì¶œ(LCEL ì²´ì¸ ì‚¬ìš©)."""
        if not contexts:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."

        try:
            model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
            print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... ({len(contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸, ëª¨ë¸: {model_label})")

            # âœ… ì»¨í…ìŠ¤íŠ¸ íŠ¸ë ì¼€ì´ì…˜ (ìœ ì‚¬ë„ ìˆœ)
            context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)

            # LCEL: prompt â†’ llm â†’ parser
            used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
            chain = self.prompt | used_llm | self.parser
            answer: str = chain.invoke({"question": query, "context": context_text})

            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # ========================= í†µí•© RAG ì²˜ë¦¬ =========================

    def query_rag(
        self, query: str, top_k: int = 4, model: str | None = None, show_sources: bool = True
    ) -> str:
        """ì§ˆì˜ â†’ ê²€ìƒ‰ â†’ ìƒì„±ê¹Œì§€ í†µí•© ì‹¤í–‰"""
        print(f"\nğŸ” ì§ˆì˜: {query}")

        contexts = self.retrieve_documents(query, top_k)
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        answer = self.generate_answer(query, contexts, model)

        if show_sources:
            sources = []
            for _, meta in contexts:
                src = f"- {meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} (ì²­í¬ {meta.get('chunk_idx', 'N/A')})"
                if src not in sources:
                    sources.append(src)
            return f"{answer}\n\nğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)

        return answer

    # ========================= ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ =========================

    def interactive_mode(self):
        print("\nğŸ¯ ëŒ€í™”í˜• RAG ê²€ìƒ‰ ì‹œì‘! (ì¢…ë£Œ: ë¹ˆ ì¤„)")
        print("-" * 60)
        while True:
            try:
                q = input("\n> ").strip()
                if not q:
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                print("\n=== ë‹µë³€ ===")
                print(self.query_rag(q))
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")


# ========================= LangChain ë„êµ¬ ì •ì˜ =========================
_retrieve_service = RetrieveService()

@tool
def rag_search_tool(query: str) -> str:
    """
    ë‚´ë¶€ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” RAG ë„êµ¬ì…ë‹ˆë‹¤.
    ì‚¬ë‚´ ë¬¸ì„œ, ì •ì±…, ì ˆì°¨, ê°€ì´ë“œë¼ì¸ ë“±ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ“š RAG ë„êµ¬ ì‹¤í–‰: '{query}'")
    return _retrieve_service.query_rag(query, top_k=3)


rag_tools = [rag_search_tool]


# ========================= í¸ì˜ í•¨ìˆ˜ë“¤ =========================

def retrieve_documents(query: str, top_k: int = 3) -> List[Tuple[str, dict]]:
    return RetrieveService().retrieve_documents(query, top_k)


def generate_answer(query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
    return RetrieveService().generate_answer(query, contexts, model)


def query_rag(query: str, top_k: int = 4, model: str | None = None) -> str:
    return RetrieveService().query_rag(query, top_k, model)


# ========================= CLI =========================

def _healthcheck_vectorstore() -> bool:
    """âœ… private ì†ì„± ì ‘ê·¼ ì—†ì´ í—¬ìŠ¤ì²´í¬: ë”ë¯¸ ê²€ìƒ‰ ì‹œë„"""
    try:
        _ = _vectorstore.similarity_search("__healthcheck__", k=1)
        return True
    except Exception as e:
        print(f"âŒ Chroma í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return False


def main():
    print("=" * 80)
    print("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤ (LangChain ê¶Œì¥ì‚¬í•­ ë°˜ì˜)")
    print("=" * 80)

    print(f"ğŸ“ CHROMA_PATH: {CHROMA_PATH}")
    print(f"ğŸ—„ï¸ COLLECTION_NAME: {COLLECTION_NAME}")
    print(f"ğŸ”¤ EMBED_MODEL: {EMBED_MODEL}")
    print(f"ğŸ§  CHAT_MODEL: {CHAT_MODEL}")
    print(f"ğŸ§» MAX_CONTEXT_CHARS: {MAX_CONTEXT_CHARS}")

    if not _healthcheck_vectorstore():
        print("ë¨¼ì € ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì ì¬í•˜ì„¸ìš”.")
        return

    RetrieveService().interactive_mode()


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°„ë‹¨ ì§ˆì˜ 3ê°œ ì‹¤í–‰")
        for q in ["ê¸°ë¡ë¬¼ ê´€ë¦¬", "ê´€ë¦¬ê¸°ì¤€í‘œê°€ ë­ì•¼?", "ì•¼ê°„ ë° íœ´ì¼ê·¼ë¡œ ê´€ë ¨ ê·œì • ì•Œë ¤ì¤˜"]:
            print("\nQ:", q)
            print(query_rag(q))
            print("-" * 60)
    else:
        main()
