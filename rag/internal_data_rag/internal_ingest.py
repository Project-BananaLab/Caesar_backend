# -*- coding: utf-8 -*-
# ë¬¸ì„œ ì„ë² ë”© ë° ChromaDB ì €ì¥ ì„œë¹„ìŠ¤
# internal_ingest.py

import os
import sys
import time
import zipfile
from pathlib import Path
from typing import List, Tuple, Optional

import pdfplumber
import docx
import openpyxl
from dotenv import load_dotenv

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# Chroma/Collection
CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data")

# ì²­í‚¹ íŒŒë¼ë¯¸í„° (í•„ìš”ì‹œ .envë¡œ ì¡°ì ˆ)
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))       # ì²­í¬ í¬ê¸°
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))  # ì˜¤ë²„ë©

# ì—‘ì…€ í­ë°œ ë°©ì§€ ì˜µì…˜
XLSX_MAX_ROWS_PER_SHEET = int(os.getenv("XLSX_MAX_ROWS_PER_SHEET", "10000"))
XLSX_MAX_COLS_PER_SHEET = int(os.getenv("XLSX_MAX_COLS_PER_SHEET", "512"))     # ğŸ”¹ ì¶”ê°€: ì—´ ìƒí•œ ìº¡
XLSX_SKIP_HIDDEN_SHEETS = os.getenv("XLSX_SKIP_HIDDEN_SHEETS", "true").lower() == "true"

# ì„ë² ë”© ìš”ì²­ ë°°ì¹˜ í•œë„ (ìš”ì²­ë‹¹ í† í° ìƒí•œ 300k ëŒ€ë¹„ ì—¬ìœ )
EMBED_MAX_TOKENS_PER_REQUEST = int(os.getenv("EMBED_MAX_TOKENS_PER_REQUEST", "280000"))
EMBED_MAX_ITEMS_PER_REQUEST = int(os.getenv("EMBED_MAX_ITEMS_PER_REQUEST", "256"))

# tiktokenì€ ì„ íƒì 
try:
    import tiktoken
    _TIKTOKEN_ENC = tiktoken.get_encoding("cl100k_base")
except Exception:
    _TIKTOKEN_ENC = None

# OpenAI
client = OpenAI()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì‹¤ì œ Office Open XML í¬ë§· ìŠ¤ë‹ˆí•‘(.docx/.xlsx êµ¬ë¶„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _detect_office_kind(path: Path) -> Optional[str]:
    """
    ZIP ê¸°ë°˜ Office ë¬¸ì„œì˜ ì‹¤ì œ ì¢…ë¥˜ë¥¼ ì¶”ì •:
      - 'docx'  : word/document.xml ì¡´ì¬
      - 'xlsx'  : xl/workbook.xml ì¡´ì¬
      - None    : ZIP ì•„ë‹˜ ë˜ëŠ” Office OpenXML ì•„ë‹˜
    """
    try:
        if not zipfile.is_zipfile(path):
            return None
        with zipfile.ZipFile(path) as z:
            names = set(z.namelist())
        if any(n.startswith("word/") for n in names):
            return "docx"
        if any(n.startswith("xl/") for n in names):
            return "xlsx"
        return None
    except Exception:
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ë² ë”© ë°°ì¹˜ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _estimate_tokens(text: str) -> int:
    """ì„ë² ë”© í† í° ëŒ€ëµì¹˜. tiktoken ìˆìœ¼ë©´ ì •í™•, ì—†ìœ¼ë©´ ë¬¸ììˆ˜/4 ê·¼ì‚¬."""
    if _TIKTOKEN_ENC is not None:
        try:
            return len(_TIKTOKEN_ENC.encode(text))
        except Exception:
            pass
    return max(1, len(text) // 4)


def embed_texts_batched(texts: List[str]) -> List[List[float]]:
    """í† í°/ì•„ì´í…œ ì˜ˆì‚°ì„ ì§€ì¼œê°€ë©° ì—¬ëŸ¬ ë²ˆìœ¼ë¡œ ë‚˜ëˆ  ì„ë² ë”©."""
    if not texts:
        return []

    batches: List[List[str]] = []
    current: List[str] = []
    current_tokens = 0

    for t in texts:
        tk = _estimate_tokens(t)

        # ë‹¨ì¼ ì²­í¬ê°€ ì˜ˆì‚°ì„ ë„˜ë”ë¼ë„(ê±°ì˜ ì—†ì§€ë§Œ) ë‹¨ë… ë°°ì¹˜ë¡œ ë³´ëƒ„
        if tk > EMBED_MAX_TOKENS_PER_REQUEST:
            if current:
                batches.append(current)
                current, current_tokens = [], 0
            batches.append([t])
            continue

        if current and (
            current_tokens + tk > EMBED_MAX_TOKENS_PER_REQUEST
            or len(current) >= EMBED_MAX_ITEMS_PER_REQUEST
        ):
            batches.append(current)
            current, current_tokens = [], 0

        current.append(t)
        current_tokens += tk

    if current:
        batches.append(current)

    all_embeddings: List[List[float]] = []
    for i, batch in enumerate(batches, 1):
        print(f"  ğŸ” ì„ë² ë”© ë°°ì¹˜ {i}/{len(batches)} (items={len(batch)}) ìš”ì²­ ì¤‘...")
        resp = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        all_embeddings.extend([d.embedding for d in resp.data])

    return all_embeddings


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class IngestService:
    """ë¬¸ì„œ ì„ë² ë”© ë° ChromaDB ì €ì¥ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
        self.supported_extensions = {".pdf", ".docx", ".xlsx"}

    # ========================= íŒŒì¼ íŒŒì‹± =========================
    def read_pdf(self, path: Path) -> str:  # PDF íŒŒì¼ íŒŒì‹±
        texts = []
        try:
            with pdfplumber.open(str(path)) as pdf:
                for page in pdf.pages:
                    t = page.extract_text() or ""
                    if t.strip():
                        texts.append(t)
        except Exception as e:
            raise ValueError(f"PDF ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return "\n\n".join(texts)

    def read_docx(self, path: Path) -> str:  # DOCX íŒŒì¼ íŒŒì‹±
        try:
            d = docx.Document(str(path))
        except Exception as e:
            raise ValueError(f"DOCX ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        acc: List[str] = []
        acc.extend([p.text for p in d.paragraphs if p.text and p.text.strip()])
        # í…Œì´ë¸” ì¶”ì¶œ(ê°„ë‹¨)
        for table in d.tables:
            for row in table.rows:
                cells = [c.text.strip() for c in row.cells]
                if any(cells):
                    acc.append(" | ".join(cells))
        return "\n".join(acc)

    def read_xlsx(self, path: Path) -> str:  # XLSX íŒŒì¼ íŒŒì‹± (í­ì£¼ ë°©ì§€ íŠ¸ë¦¬ë°/ìº¡ ì ìš©)
        try:
            wb = openpyxl.load_workbook(str(path), data_only=True, read_only=True)
        except Exception as e:
            # ì•”í˜¸í™”/ì†ìƒ/ë¹„ì •ìƒ êµ¬ì¡° ë“± ëª…í™•í•œ ë©”ì‹œì§€ ì „ë‹¬
            raise ValueError(f"ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

        if not wb.worksheets:
            raise ValueError("ì—‘ì…€ì— ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        acc: List[str] = []
        for ws in wb.worksheets:
            # ìˆ¨ê¹€ ì‹œíŠ¸ ìŠ¤í‚µ ì˜µì…˜
            try:
                if XLSX_SKIP_HIDDEN_SHEETS and getattr(ws, "sheet_state", "visible") != "visible":
                    continue
            except Exception:
                pass

            acc.append(f"\n### [Sheet] {ws.title}")
            rows = 0

            # ğŸ”¹ ì—´ ìƒí•œ ìº¡ì„ openpyxl ë ˆë²¨ì—ì„œ ë°”ë¡œ ì ìš©
            iter_kwargs = {"values_only": True}
            if XLSX_MAX_COLS_PER_SHEET and XLSX_MAX_COLS_PER_SHEET > 0:
                iter_kwargs["max_col"] = XLSX_MAX_COLS_PER_SHEET

            for row in ws.iter_rows(**iter_kwargs):
                if rows >= XLSX_MAX_ROWS_PER_SHEET:
                    acc.append(f"...(truncated at {XLSX_MAX_ROWS_PER_SHEET} rows)")
                    break

                # ğŸ”¹ í–‰ ìš°ì¸¡ì˜ ë¹ˆ ì—´ íŠ¸ë¦¬ë°: ì‹¤ì œ ê°’ì´ ìˆëŠ” ë§ˆì§€ë§‰ ì—´ê¹Œì§€ë§Œ ì‚¬ìš©
                last = -1
                # (ì—´ ìº¡ì´ ì ìš©ëœ ë²”ìœ„ ë‚´ì—ì„œë§Œ ê²€ì‚¬)
                for i, v in enumerate(row):
                    sv = (str(v).strip() if v is not None else "")
                    if sv != "":
                        last = i

                if last < 0:
                    continue  # ì™„ì „ ë¹ˆ í–‰ì€ ìŠ¤í‚µ

                # ğŸ”¹ ìµœì¢… ì‚¬ìš©í•  ì—´ í­ ê²°ì •
                width = last + 1
                if XLSX_MAX_COLS_PER_SHEET and XLSX_MAX_COLS_PER_SHEET > 0:
                    width = min(width, XLSX_MAX_COLS_PER_SHEET)

                # ğŸ”¹ ìµœì¢… ë¬¸ìì—´ êµ¬ì„±
                row_vals = []
                for v in row[:width]:
                    row_vals.append("" if v is None else str(v).strip())

                acc.append(" | ".join(row_vals))
                rows += 1

        return "\n".join(acc)

    def load_text(self, file_path: str, verbose: bool = True) -> str:
        """í™•ì¥ì + ì‹¤ì œ í¬ë§· ìŠ¤ë‹ˆí•‘ìœ¼ë¡œ ì ì ˆí•œ íŒŒì„œ ì„ íƒ"""
        p = Path(file_path)
        ext = p.suffix.lower()

        if verbose:
            print(f"  ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘: {p.name} ({ext})")

        actual = _detect_office_kind(p)  # ì‹¤ì œ í¬ë§· ìŠ¤ë‹ˆí•‘(ZIP ê¸°ë°˜ Office ë¬¸ì„œì˜ ì‹¤ì œ ì¢…ë¥˜ë¥¼ ì¶”ì •)

        try:
            if ext == ".pdf":   # PDF íŒŒì¼ íŒŒì‹±
                return self.read_pdf(p)

            if ext == ".docx" or (actual == "docx" and ext != ".xlsx"):  # DOCX íŒŒì¼ íŒŒì‹±
                if verbose and ext != ".docx" and actual == "docx":
                    print("  âš ï¸ í™•ì¥ìì™€ ë‹¤ë¥¸ ì‹¤ì œ í¬ë§·(docx) ê°ì§€ â†’ docx íŒŒì„œ ì‚¬ìš©")
                return self.read_docx(p)

            if ext == ".xlsx" or (actual == "xlsx" and ext != ".docx"):  # XLSX íŒŒì¼ íŒŒì‹±
                if verbose and ext != ".xlsx" and actual == "xlsx":
                    print("  âš ï¸ í™•ì¥ìì™€ ë‹¤ë¥¸ ì‹¤ì œ í¬ë§·(xlsx) ê°ì§€ â†’ xlsx íŒŒì„œ ì‚¬ìš©")
                return self.read_xlsx(p)

            # ë§ˆì§€ë§‰ ë³´ë£¨: ì‹¤ì œ í¬ë§· ê¸°ì¤€ ì‹œë„
            if actual == "docx":
                if verbose:
                    print("  âš ï¸ í™•ì¥ì ë¯¸ì§€ì›/ë¶ˆëª…ì´ë‚˜ ì‹¤ì œ í¬ë§·(docx) ê°ì§€ â†’ docx íŒŒì„œ ì‚¬ìš©")
                return self.read_docx(p)
            if actual == "xlsx":
                if verbose:
                    print("  âš ï¸ í™•ì¥ì ë¯¸ì§€ì›/ë¶ˆëª…ì´ë‚˜ ì‹¤ì œ í¬ë§·(xlsx) ê°ì§€ â†’ xlsx íŒŒì„œ ì‚¬ìš©")
                return self.read_xlsx(p)

            if verbose:
                print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext} (ì‹¤ì œ í¬ë§· ë¯¸í™•ì¸)")
            return ""

        except Exception as e:
            if verbose:
                print(f"  âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({p.name}): {e}")
            return ""

    # ========================= Chroma í—¬í¼ =========================
    def get_chroma_collection(self):    # ChromaDB ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±
        try:
            # ChromaDB ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)
            chroma = chromadb.PersistentClient(
                path=CHROMA_PATH,
                settings=Settings(
                    anonymized_telemetry=False,
                    is_persistent=True,
                ),
            )
            return chroma.get_or_create_collection(name=COLLECTION_NAME)
        except Exception as e:
            print(f"ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            print("ìƒˆë¡œìš´ ChromaDB ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¬ì‹œë„ ì¤‘...")
            chroma = chromadb.Client()
            return chroma.get_or_create_collection(name=COLLECTION_NAME)

    # ========================= ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ =========================
    def ingest_single_file(self, file_path: str, show_preview: bool = True) -> bool:
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {file_path}")
        try:
            # 1) íŒŒì¼ ë¡œë“œ
            raw_text = self.load_text(file_path)
            if not raw_text.strip():
                print(f"âŒ ë¹ˆ íŒŒì¼ì´ê±°ë‚˜ ì½ê¸° ì‹¤íŒ¨: {file_path}")
                return False

            print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ, ì „ì²´ ê¸¸ì´: {len(raw_text):,} chars")

            # 2) í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.text_splitter.split_text(raw_text)

            # ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶œë ¥
            for i, c in enumerate(chunks):
                print(f"  [Chunk {i}] {len(c):,} chars")

            print(f"ğŸª“ ì²­í‚¹ ì™„ë£Œ â†’ ì´ {len(chunks)} chunks")
            if not chunks:
                print("âŒ ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                return False

            # ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
            if show_preview:
                for i, c in enumerate(chunks[:3]):
                    print(f"  [Chunk {i}] {c[:100]}...")

            # 3) ì„ë² ë”© ìƒì„±
            print("âš™ï¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = embed_texts_batched(chunks)
            if not embeddings:
                print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨(ë¹ˆ ì…ë ¥).")
                return False
            print(f"âœ… ì„ë² ë”© ì™„ë£Œ â†’ shape: {len(embeddings)} x {len(embeddings[0])}")

            # 4) ChromaDB ì €ì¥
            collection = self.get_chroma_collection()

            # ê¸°ì¡´ ë™ì¼ íŒŒì¼ ì²­í¬ ì‚­ì œ(ì¤‘ë³µ ë°©ì§€)
            file_name = Path(file_path).name
            existing = collection.get(where={"source": file_name})
            if existing and existing.get("ids"):
                collection.delete(ids=existing["ids"])
                print(f"ğŸ—‘ ê¸°ì¡´ {len(existing['ids'])} ì²­í¬ ì‚­ì œ")

            # ìƒˆ ë°ì´í„° ì¶”ê°€
            base_id = Path(file_path).stem
            ids = [f"{base_id}-{i}" for i in range(len(chunks))]
            metadatas = [{"source": file_name, "chunk_idx": i} for i in range(len(chunks))]

            collection.add(
                ids=ids,
                metadatas=metadatas,
                embeddings=embeddings,
                documents=chunks,
            )

            print(f"ğŸ‰ ì™„ë£Œ! {len(chunks)} chunks â†’ Chroma collection '{COLLECTION_NAME}' ì €ì¥")
            return True

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False

    # ========================= ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬ =========================
    def get_supported_files(self, folder_path: Path) -> List[Path]:  # ì§€ì›ë˜ëŠ” íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
        files: List[Path] = []
        for file_path in folder_path.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                files.append(file_path)
        return sorted(files)    # ì •ë ¬ëœ íŒŒì¼ ëª©ë¡ ë°˜í™˜(íŒŒì¼ëª… ìˆœ)

    def process_single_file_batch(self, file_path: Path, collection) -> Tuple[int, bool]:  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {file_path.name}")                    #(ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜, ì„±ê³µ ì—¬ë¶€)
        try:
            # 1) íŒŒì¼ ë¡œë“œ
            raw_text = self.load_text(str(file_path))
            if not raw_text.strip():
                print(f"  âš ï¸ ë¹ˆ íŒŒì¼ì´ê±°ë‚˜ ì½ê¸° ì‹¤íŒ¨: {file_path.name}")
                return 0, False

            print(f"  âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ, ì „ì²´ ê¸¸ì´: {len(raw_text):,} chars")

            # 2) í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.text_splitter.split_text(raw_text)

            # ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶œë ¥
            for i, c in enumerate(chunks):
                print(f"  [Chunk {i}] {len(c):,} chars")

            print(f"  ğŸª“ ì²­í‚¹ ì™„ë£Œ â†’ ì´ {len(chunks)} chunks")
            if not chunks:
                print(f"  âš ï¸ ì²­í‚¹ ê²°ê³¼ê°€ ì—†ìŒ: {file_path.name}")
                return 0, False

            # 3) ì„ë² ë”© ìƒì„±
            print(f"  âš™ï¸ ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)} chunks)")
            embeddings = embed_texts_batched(chunks)
            if not embeddings:
                print("  âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨(ë¹ˆ ì…ë ¥)")
                return 0, False
            print(f"  âœ… ì„ë² ë”© ì™„ë£Œ â†’ shape: {len(embeddings)} x {len(embeddings[0])}")

            # 4) ê¸°ì¡´ ì²­í¬ ì‚­ì œ(ì¤‘ë³µ ë°©ì§€)
            file_name = file_path.name
            existing = collection.get(where={"source": file_name})
            if existing and existing.get("ids"):
                collection.delete(ids=existing["ids"])
                print(f"  ğŸ—‘ ê¸°ì¡´ {len(existing['ids'])} ì²­í¬ ì‚­ì œ")

            # 5) ìƒˆ ë°ì´í„° ì¶”ê°€
            base_id = file_path.stem
            ids = [f"{base_id}-{i}" for i in range(len(chunks))]
            metadatas = [{"source": file_name, "chunk_idx": i} for i in range(len(chunks))]

            collection.add(
                ids=ids,
                metadatas=metadatas,
                embeddings=embeddings,
                documents=chunks,
            )

            print(f"  ğŸ‰ ì €ì¥ ì™„ë£Œ! {len(chunks)} chunks â†’ ChromaDB")
            return len(chunks), True

        except Exception as e:
            print(f"  âŒ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path.name}): {str(e)}")
            return 0, False

    # í´ë” ë‚´ ëª¨ë“  ì§€ì›ë˜ëŠ” íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ChromaDBì— ì €ì¥
    def ingest_multiple_files(self, folder_path: str, clear_collection: bool = False) -> dict:
        folder = Path(folder_path)  # folder_path (str): ì²˜ë¦¬í•  í´ë” ê²½ë¡œ, clear_collection (bool): ì²˜ë¦¬ ì „ ì»¬ë ‰ì…˜ ì „ì²´ ì‚­ì œ ì—¬ë¶€ -> dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„(ì„±ê³µ/ì‹¤íŒ¨ íŒŒì¼ ìˆ˜, ì´ ì²­í¬ ìˆ˜, ì†Œìš” ì‹œê°„, ì»¬ë ‰ì…˜ ì´ë¦„)
        if not folder.exists() or not folder.is_dir():
            print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
            return {"success": False, "error": "í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}

        print(f"ğŸ“‚ í´ë” ì²˜ë¦¬ ì‹œì‘: {folder.absolute()}")

        files_to_process = self.get_supported_files(folder) # ì§€ì›ë˜ëŠ” íŒŒì¼ë“¤ ì°¾ê¸°
        if not files_to_process:
            print("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì§€ì› í˜•ì‹: .pdf, .docx, .xlsx)")
            return {"success": False, "error": "ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŒ"}

        print(f"ğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ {len(files_to_process)}ê°œ:")
        for i, file_path in enumerate(files_to_process, 1):
            file_size = file_path.stat().st_size
            size_mb = file_size / (1024 * 1024)
            print(f"  {i:2d}. {file_path.name} ({size_mb:.1f}MB)")

        print(f"\nğŸ”§ ChromaDB ì´ˆê¸°í™” ì¤‘... (ê²½ë¡œ: {CHROMA_PATH})")
        collection = self.get_chroma_collection()

        # ì»¬ë ‰ì…˜ ì „ì²´ ì‚­ì œ ì˜µì…˜
        if clear_collection:
            try:
                existing_count = collection.count()
                if existing_count > 0:
                    print(f"ğŸ—‘ ê¸°ì¡´ ì»¬ë ‰ì…˜ ë°ì´í„° ì „ì²´ ì‚­ì œ ì¤‘... ({existing_count} items)")
                    all_data = collection.get()
                    if all_data.get("ids"):
                        collection.delete(ids=all_data["ids"])
                    print("âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # íŒŒì¼ë³„ ì²˜ë¦¬ í†µê³„
        total_chunks = 0
        successful_files = 0
        failed_files = 0
        start_time = time.time()

        print(f"\nğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘... (ì´ {len(files_to_process)}ê°œ)")
        print("=" * 60)

        for i, file_path in enumerate(files_to_process, 1):
            print(f"\n[{i}/{len(files_to_process)}] {file_path.name}")

            chunks_count, success = self.process_single_file_batch(file_path, collection)

            if success:
                successful_files += 1
                total_chunks += chunks_count
            else:
                failed_files += 1

            progress = (i / len(files_to_process)) * 100    # ì§„í–‰ë¥  í‘œì‹œ
            print(f"  ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(files_to_process)})")

            if i < len(files_to_process):   # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)

        elapsed_time = time.time() - start_time

        result = {
            "success": True,
            "successful_files": successful_files,
            "failed_files": failed_files,
            "total_chunks": total_chunks,
            "elapsed_time": elapsed_time,
            "collection_name": COLLECTION_NAME,
        }

        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"  âœ… ì„±ê³µ: {successful_files}ê°œ íŒŒì¼")
        print(f"  âŒ ì‹¤íŒ¨: {failed_files}ê°œ íŒŒì¼")
        print(f"  ğŸ“ ì´ ì²­í¬ ìˆ˜: {total_chunks:,}ê°œ")
        print(f"  â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"  ğŸ—„ï¸ ì»¬ë ‰ì…˜: '{COLLECTION_NAME}'")
        print("=" * 60)

        return result


# ========================= í¸ì˜ í•¨ìˆ˜ =========================
# ë‹¨ì¼ íŒŒì¼ ì„ë² ë”© í¸ì˜ í•¨ìˆ˜
def ingest_single_file(file_path: str, show_preview: bool = True) -> bool:
    return IngestService().ingest_single_file(file_path, show_preview)

# ë‹¤ì¤‘ íŒŒì¼ ì„ë² ë”© í¸ì˜ í•¨ìˆ˜
def ingest_multiple_files(folder_path: str, clear_collection: bool = False) -> dict:
    return IngestService().ingest_multiple_files(folder_path, clear_collection)


# ========================= CLI =========================
def main():
    print("=" * 80)
    print("ğŸ“š ë¬¸ì„œ ì„ë² ë”© ì„œë¹„ìŠ¤")
    print("=" * 80)

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  ë‹¨ì¼ íŒŒì¼: python ingest_service.py <íŒŒì¼ê²½ë¡œ>")
        print("  ë‹¤ì¤‘ íŒŒì¼: python ingest_service.py <í´ë”ê²½ë¡œ> [--clear]")
        print("\nì˜µì…˜:")
        print("  --clear: ì²˜ë¦¬ ì „ ê¸°ì¡´ ì»¬ë ‰ì…˜ ë°ì´í„° ì „ì²´ ì‚­ì œ")
        print("\nì˜ˆì‹œ:")
        print("  python ingest_service.py ./rag/inside_data_rag/data/document.pdf")
        print("  python ingest_service.py ./rag/inside_data_rag/data")
        print("  python ingest_service.py ./rag/inside_data_rag/data --clear")
        sys.exit(1)

    path = sys.argv[1]
    clear_collection = "--clear" in sys.argv

    if clear_collection:
        print("âš ï¸ --clear ì˜µì…˜ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ê°€ ëª¨ë‘ ì‚­ì œë©ë‹ˆë‹¤.")
        confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm not in ["y", "yes"]:
            print("âŒ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)

    try:
        path_obj = Path(path)

        if path_obj.is_file():  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
            print("ğŸ“„ ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ")
            success = ingest_single_file(path)
            sys.exit(0 if success else 1)

        elif path_obj.is_dir():  # ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬
            print("ğŸ“‚ ë‹¤ì¤‘ íŒŒì¼ ëª¨ë“œ")
            result = ingest_multiple_files(path, clear_collection)
            sys.exit(0 if result["success"] else 1)

        else:
            print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
